"""
çŸ¥è¯†å›¾è°±æå–å™¨ - ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æå–
"""
import logging
import json
from typing import Dict, List, Any, Tuple
import aiohttp
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
 
from backend.config.settings import settings
 
logger = logging.getLogger(__name__)


class KnowledgeGraphExtractor:
    """åŸºäºLLMçš„çŸ¥è¯†å›¾è°±æå–å™¨"""
    
    def __init__(self):
        # ä½¿ç”¨ç¡…åŸºæµåŠ¨APIé…ç½®
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.api_key = settings.siliconflow_api_key
        self.model = settings.siliconflow_model or "Qwen/Qwen2.5-72B-Instruct"
        
        # å®šä¹‰å®ä½“ç±»å‹ï¼ˆé’ˆå¯¹é‡‘èæ–‡æ¡£ï¼‰
        self.entity_types = [
            "COMPANY",       # å…¬å¸åç§°
            "PERSON",        # äººç‰©ï¼ˆCEOã€é«˜ç®¡ï¼‰
            "METRIC",        # è´¢åŠ¡æŒ‡æ ‡ï¼ˆrevenue, profitç­‰ï¼‰
            "NUMBER",        # æ•°å€¼
            "DATE",          # æ—¥æœŸ
            "PRODUCT",       # äº§å“/æœåŠ¡
            "LOCATION",      # åœ°ç‚¹
            "EVENT",         # äº‹ä»¶ï¼ˆå¹¶è´­ã€å‘å¸ƒç­‰ï¼‰
        ]
        
        # å®šä¹‰å…³ç³»ç±»å‹
        self.relation_types = [
            "IS_CEO_OF",
            "HAS_REVENUE",
            "ACQUIRED",
            "LAUNCHED",
            "LOCATED_IN",
            "COMPETES_WITH",
            "OWNS",
            "INCREASED_BY",
            "DECREASED_BY",
            "ANNOUNCED"
        ]
    
    async def extract_from_text(self, text: str, chunk_size: int = 3000) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMä»æ–‡æœ¬æå–çŸ¥è¯†å›¾è°±
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: åˆ†å—å¤§å°ï¼ˆé¿å…è¶…è¿‡tokené™åˆ¶ï¼‰
        
        Returns:
            {
                "entities": List[Dict],
                "relationships": List[Dict]
            }
        """
        logger.info("ğŸ§  Extracting knowledge graph using LLM...")
        
        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œåˆ†å—å¤„ç†
        chunks = self._split_text(text, chunk_size)
        logger.info(f"Split into {len(chunks)} chunks")
        
        all_entities = []
        all_relationships = []
        
        # å¹¶å‘å¤„ç†å¤šä¸ªå—ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
        for i in range(0, len(chunks), 3):  # æ¯æ¬¡å¤„ç†3ä¸ªå—
            batch = chunks[i:i+3]
            
            tasks = [self._extract_from_chunk(chunk, idx + i) for idx, chunk in enumerate(batch)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Chunk extraction failed: {result}")
                    continue
                
                all_entities.extend(result["entities"])
                all_relationships.extend(result["relationships"])
        
        # å»é‡å’Œåˆå¹¶
        entities = self._deduplicate_entities(all_entities)
        relationships = self._deduplicate_relationships(all_relationships)
        
        logger.info(f"âœ… Extracted {len(entities)} unique entities, {len(relationships)} unique relationships")
        
        return {
            "entities": entities,
            "relationships": relationships
        }
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def _extract_from_chunk(self, text: str, chunk_id: int) -> Dict:
        """ä»å•ä¸ªæ–‡æœ¬å—æå–"""
        
        prompt = f"""
You are a knowledge graph extraction expert for financial documents.

Extract entities and relationships from the following text.

**Entity Types to Extract:**
- COMPANY: Company names
- PERSON: Names of executives, CEOs, etc.
- METRIC: Financial metrics (revenue, profit, EPS, etc.)
- NUMBER: Numerical values with context
- DATE: Dates and time periods
- PRODUCT: Products or services
- LOCATION: Cities, countries, regions
- EVENT: Significant events (acquisitions, launches, etc.)

**Relationship Types to Extract:**
- IS_CEO_OF: Person is CEO of Company
- HAS_REVENUE: Company has revenue of Number
- ACQUIRED: Company acquired Company
- LAUNCHED: Company launched Product
- LOCATED_IN: Company located in Location
- ANNOUNCED: Company announced Event
- INCREASED_BY / DECREASED_BY: Metric changed by Number

**TEXT:**
{text[:2500]}

**OUTPUT FORMAT (JSON):**
{{
  "entities": [
    {{"text": "Apple Inc", "type": "COMPANY"}},
    {{"text": "Tim Cook", "type": "PERSON"}},
    {{"text": "$95.3 billion", "type": "NUMBER", "context": "revenue"}}
  ],
  "relationships": [
    {{"source": "Tim Cook", "target": "Apple Inc", "relation": "IS_CEO_OF"}},
    {{"source": "Apple Inc", "target": "$95.3 billion", "relation": "HAS_REVENUE"}}
  ]
}}

Extract ONLY clear, factual information. Focus on the most important entities and relationships.
Return valid JSON only, no additional text.
"""
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a knowledge graph extraction expert. Return only valid JSON."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,  # ä½æ¸©åº¦æé«˜å‡†ç¡®æ€§
            "max_tokens": 2000,
            "stream": False,
            "response_format": {"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"API request failed: {response.status} - {error_text}")
                        return {"entities": [], "relationships": []}
                    
                    result_data = await response.json()
                    
                    if "choices" not in result_data or not result_data["choices"]:
                        logger.error(f"No choices in response: {result_data}")
                        return {"entities": [], "relationships": []}
                    
                    result_text = result_data["choices"][0]["message"]["content"]
                    result = json.loads(result_text)
                    
                    # éªŒè¯æ ¼å¼
                    if "entities" not in result:
                        result["entities"] = []
                    if "relationships" not in result:
                        result["relationships"] = []
                    
                    logger.info(f"Chunk {chunk_id}: {len(result['entities'])} entities, {len(result['relationships'])} relationships")
                    
                    return result
                    
        except aiohttp.ClientError as e:
            logger.error(f"HTTP client error: {e}")
            raise
        except asyncio.TimeoutError:
            logger.error("Request timeout")
            return {"entities": [], "relationships": []}
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            return {"entities": [], "relationships": []}
        except Exception as e:
            logger.error(f"LLM extraction failed: {e}")
            return {"entities": [], "relationships": []}
    
    def _split_text(self, text: str, chunk_size: int) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å»é‡å®ä½“ï¼ˆåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰"""
        seen = {}
        unique = []
        
        for entity in entities:
            text = entity.get("text", "").strip().lower()
            entity_type = entity.get("type", "")
            
            if not text:
                continue
            
            # ä½¿ç”¨ (text, type) ä½œä¸ºé”®
            key = (text, entity_type)
            
            if key not in seen:
                seen[key] = True
                # ä¿ç•™åŸå§‹æ ¼å¼
                unique.append({
                    "text": entity.get("text", "").strip(),
                    "type": entity_type,
                    "context": entity.get("context", "")
                })
        
        return unique
    
    def _deduplicate_relationships(self, relationships: List[Dict]) -> List[Dict]:
        """å»é‡å…³ç³»"""
        seen = set()
        unique = []
        
        for rel in relationships:
            source = rel.get("source", "").strip().lower()
            target = rel.get("target", "").strip().lower()
            relation = rel.get("relation", "").strip()
            
            if not source or not target or not relation:
                continue
            
            key = (source, relation, target)
            
            if key not in seen:
                seen.add(key)
                unique.append({
                    "source": rel.get("source", "").strip(),
                    "target": rel.get("target", "").strip(),
                    "relation": relation,
                    "context": rel.get("context", "")
                })
        
        return unique
    
    async def extract_from_csv(self, csv_data: List[Dict]) -> Dict:
        """
        ä½¿ç”¨LLMä»CSVæå–çŸ¥è¯†å›¾è°±
        """
        logger.info("ğŸ“Š Extracting knowledge graph from CSV using LLM...")
        
        # å°†CSVè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
        text_description = self._csv_to_text(csv_data)
        
        # ä½¿ç”¨LLMæå–
        result = await self.extract_from_text(text_description)
        
        # é¢å¤–æå–åˆ—ä¹‹é—´çš„å…³ç³»
        csv_relationships = self._extract_csv_column_relationships(csv_data)
        result["relationships"].extend(csv_relationships)
        
        return result
    
    def _csv_to_text(self, csv_data: List[Dict], max_rows: int = 50) -> str:
        """å°†CSVè½¬æ¢ä¸ºæ–‡æœ¬æè¿°"""
        if not csv_data:
            return ""
        
        text = "CSV Data Analysis:\n\n"
        
        # é™åˆ¶è¡Œæ•°
        sample_data = csv_data[:max_rows]
        
        # åˆ—å
        columns = list(sample_data[0].keys()) if sample_data else []
        text += f"Columns: {', '.join(columns)}\n\n"
        
        # è½¬æ¢æ¯è¡Œä¸ºå¥å­
        for i, row in enumerate(sample_data):
            row_text = f"Row {i+1}: "
            facts = []
            for key, value in row.items():
                if value and str(value).strip():
                    facts.append(f"{key} is {value}")
            
            if facts:
                row_text += ", ".join(facts) + "."
                text += row_text + "\n"
        
        return text
    
    def _extract_csv_column_relationships(self, csv_data: List[Dict]) -> List[Dict]:
        """ä»CSVåˆ—æå–å…³ç³»"""
        if not csv_data:
            return []
        
        relationships = []
        columns = list(csv_data[0].keys())
        
        # ä¸ºæ¯ä¸€è¡Œåˆ›å»ºåˆ—ä¹‹é—´çš„å…³ç³»
        for row in csv_data[:20]:  # é™åˆ¶è¡Œæ•°
            # æ‰¾åˆ°ä¸»é”®åˆ—ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—æˆ–åŒ…å«name/idçš„åˆ—ï¼‰
            primary_key = None
            for col in columns:
                if 'name' in col.lower() or 'company' in col.lower() or 'id' in col.lower():
                    primary_key = col
                    break
            
            if not primary_key:
                primary_key = columns[0]
            
            primary_value = row.get(primary_key)
            if not primary_value:
                continue
            
            # åˆ›å»ºå…³ç³»
            for col in columns:
                if col != primary_key and row.get(col):
                    relationships.append({
                        "source": str(primary_value),
                        "target": str(row[col]),
                        "relation": f"HAS_{col.upper()}",
                        "context": f"CSV column relationship"
                    })
        
        return relationships


# å…¨å±€å®ä¾‹
kg_extractor = KnowledgeGraphExtractor()