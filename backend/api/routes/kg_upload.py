# backend/api/routes/kg_upload.py (å®Œå…¨æ›¿æ¢)
"""
çŸ¥è¯†å›¾è°±ä¸Šä¼ API - å¢å¼ºç‰ˆ
"""
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
import shutil
from pathlib import Path
import logging
from typing import Optional
 
from backend.rag.kg_generator import kg_generator
from backend.agents.document_workflow import document_workflow
from backend.rag.document_processor import document_processor
 
logger = logging.getLogger(__name__)
 
router = APIRouter(prefix="/kg", tags=["knowledge-graph"])
 
# ä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)
 
# æŠ¥å‘Šç›®å½•
REPORTS_DIR = Path("document_reports")
REPORTS_DIR.mkdir(exist_ok=True)
 
 
# backend/api/routes/kg_upload.py (ä¿®æ”¹upload_and_analyzeå‡½æ•°)
 
@router.post("/upload")
async def upload_and_analyze(
    file: UploadFile = File(...),
    generate_report: bool = True,
    generate_kg: bool = True
):
    """
    ğŸ†• å¢å¼ºç‰ˆï¼šä¸Šä¼ æ–‡ä»¶å¹¶ç”ŸæˆçŸ¥è¯†å›¾è°± + AIåˆ†ææŠ¥å‘Š
    """
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.filename.endswith(('.pdf', '.csv')):
            raise HTTPException(
                status_code=400,
                detail="Only PDF and CSV files are supported"
            )
        
        logger.info(f"ğŸ“¤ Processing file: {file.filename}")
        
        # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
        file_path = UPLOAD_DIR / file.filename
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        result = {
            "filename": file.filename,
            "file_path": str(file_path)
        }
        
        # 1. ç”ŸæˆçŸ¥è¯†å›¾è°±
        kg_result = None
        if generate_kg:
            logger.info("ğŸ—ï¸ Generating knowledge graph...")
            # ğŸ†• ç›´æ¥awaitï¼ˆä¸ç”¨asyncio.runï¼‰
            kg_result = await kg_generator.generate_from_file(str(file_path))
            result["knowledge_graph"] = kg_result
        
        # 2. ç”ŸæˆAIåˆ†ææŠ¥å‘Š
        analysis_result = None
        if generate_report:
            logger.info("ğŸ¤– Generating AI analysis report...")
            
            # æå–æ–‡æ¡£å†…å®¹
            if file.filename.endswith('.pdf'):
                doc_data = document_processor.process_pdf(str(file_path))
                document_text = doc_data["text"]
                metadata = doc_data["metadata"]
            else:  # CSV
                doc_data = document_processor.process_csv(str(file_path))
                document_text = doc_data["summary"]
                metadata = doc_data["metadata"]
            
            # è·å–çŸ¥è¯†å›¾è°±æ•°æ®ï¼ˆå¦‚æœç”Ÿæˆäº†ï¼‰
            kg_data = {}
            if kg_result:
                import json
                json_path = kg_result["output_files"]["json_path"]
                with open(json_path, 'r') as f:
                    kg_json = json.load(f)
                    kg_data = {
                        "entities": kg_json.get("entities", []),
                        "relationships": kg_json.get("relationships", [])
                    }
            
            # è¿è¡Œåˆ†æå·¥ä½œæµ
            analysis_result = await document_workflow.analyze_document(
                document_text=document_text,
                kg_data=kg_data,
                metadata=metadata
            )
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = REPORTS_DIR / f"{Path(file.filename).stem}_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(analysis_result["report"])
            
            analysis_result["report_path"] = str(report_path)
            result["analysis_report"] = analysis_result
        
        # 3. æ•´ç†è¾“å‡ºæ–‡ä»¶
        result["files"] = {}
        if kg_result:
            result["files"]["html"] = kg_result["output_files"]["html_path"]
            result["files"]["json"] = kg_result["output_files"]["json_path"]
            result["files"]["graphml"] = kg_result["output_files"]["graphml_path"]
        if analysis_result:
            result["files"]["report"] = analysis_result["report_path"]
        
        logger.info(f"âœ… Processing complete for {file.filename}")
        
        return {
            "status": "success",
            "message": "File processed successfully",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"âŒ Processing failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
 
 
@router.post("/upload-quick")
async def upload_quick_analysis(file: UploadFile = File(...)):
    """å¿«é€Ÿåˆ†æï¼ˆä»…AIæŠ¥å‘Šï¼Œä¸ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼‰"""
    return await upload_and_analyze(file, generate_report=True, generate_kg=False)
 
 
@router.post("/upload-kg-only")
async def upload_kg_only(file: UploadFile = File(...)):
    """ä»…ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼ˆä¸ç”ŸæˆAIæŠ¥å‘Šï¼‰"""
    return await upload_and_analyze(file, generate_report=False, generate_kg=True)
 
 
@router.get("/download/{filename}")
async def download_file(filename: str, type: str = "html"):
    """
    ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶
    
    Args:
        filename: åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        type: æ–‡ä»¶ç±»å‹ (html, json, graphml, report)
    """
    try:
        if type == "report":
            # ä¸‹è½½åˆ†ææŠ¥å‘Š
            report_path = REPORTS_DIR / f"{filename}_report.md"
            if not report_path.exists():
                raise HTTPException(status_code=404, detail="Report not found")
            
            return FileResponse(
                path=report_path,
                filename=f"{filename}_report.md",
                media_type="text/markdown"
            )
        else:
            # ä¸‹è½½çŸ¥è¯†å›¾è°±æ–‡ä»¶
            kg_dir = Path("knowledge_graphs")
            pattern = f"{filename}_*.{type}"
            files = list(kg_dir.glob(pattern))
            
            if not files:
                raise HTTPException(
                    status_code=404,
                    detail=f"No {type} file found"
                )
            
            latest_file = max(files, key=lambda p: p.stat().st_mtime)
            
            return FileResponse(
                path=latest_file,
                filename=latest_file.name
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Download failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
 
 
@router.get("/list")
async def list_all_documents():
    """åˆ—å‡ºæ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£"""
    try:
        documents = []
        
        # ä»çŸ¥è¯†å›¾è°±ç›®å½•è¯»å–
        kg_dir = Path("knowledge_graphs")
        for json_file in kg_dir.glob("*.json"):
            import json
            with open(json_file, 'r') as f:
                data = json.load(f)
                
                file_stem = json_file.stem.rsplit('_', 2)[0]  # ç§»é™¤æ—¶é—´æˆ³
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ææŠ¥å‘Š
                report_path = REPORTS_DIR / f"{file_stem}_report.md"
                has_report = report_path.exists()
                
                documents.append({
                    "filename": file_stem,
                    "created_at": data.get("created_at"),
                    "entities": data.get("stats", {}).get("num_entities"),
                    "relationships": data.get("stats", {}).get("num_relationships"),
                    "has_kg": True,
                    "has_report": has_report,
                    "files": {
                        "json": str(json_file),
                        "html": str(json_file.with_suffix('.html')),
                        "report": str(report_path) if has_report else None
                    }
                })
        
        return {
            "count": len(documents),
            "documents": documents
        }
        
    except Exception as e:
        logger.error(f"âŒ List failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
 
 
@router.get("/report/{filename}")
async def get_report(filename: str):
    """
    è·å–åˆ†ææŠ¥å‘Šå†…å®¹ï¼ˆè¿”å›Markdownï¼‰
    """
    try:
        report_path = REPORTS_DIR / f"{filename}_report.md"
        
        if not report_path.exists():
            raise HTTPException(status_code=404, detail="Report not found")
        
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return {
            "filename": filename,
            "content": content,
            "path": str(report_path)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Get report failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
 
 
@router.delete("/delete/{filename}")
async def delete_document(filename: str):
    """
    åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰ç›¸å…³æ–‡ä»¶
    """
    try:
        deleted_files = []
        
        # åˆ é™¤ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
        upload_file = UPLOAD_DIR / filename
        if upload_file.exists():
            upload_file.unlink()
            deleted_files.append(str(upload_file))
        
        # åˆ é™¤çŸ¥è¯†å›¾è°±æ–‡ä»¶
        kg_dir = Path("knowledge_graphs")
        file_stem = Path(filename).stem
        for kg_file in kg_dir.glob(f"{file_stem}_*"):
            kg_file.unlink()
            deleted_files.append(str(kg_file))
        
        # åˆ é™¤åˆ†ææŠ¥å‘Š
        report_file = REPORTS_DIR / f"{file_stem}_report.md"
        if report_file.exists():
            report_file.unlink()
            deleted_files.append(str(report_file))
        
        return {
            "status": "success",
            "message": f"Deleted {len(deleted_files)} files",
            "deleted_files": deleted_files
        }
        
    except Exception as e:
        logger.error(f"âŒ Delete failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))